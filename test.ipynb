{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from data import ModelNet40\n",
    "from data import Bunny\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from model import PRNet\n",
    "from types import SimpleNamespace\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nuscenesHelper.utils import scale_to_range, filter_pointcloud\n",
    "from nuscenesHelper import LidarDataset\n",
    "from nuscenesHelper import NuScenesHelper\n",
    "from nuscenes.nuscenes import NuScenes, NuScenesExplorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = SimpleNamespace(\n",
    "    emb_nn='dgcnn', \n",
    "    attention='transformer', \n",
    "    head='svd',\n",
    "    svd_on_gpu=True,\n",
    "    n_emb_dims=512,\n",
    "    n_blocks=1,\n",
    "    n_heads=4,\n",
    "    n_iters=3,\n",
    "    discount_factor=0.9,\n",
    "    n_ff_dims=1024,\n",
    "    n_keypoints=512,\n",
    "    temp_factor=100,\n",
    "    cat_sampler='gumbel_softmax',\n",
    "    dropout=0,\n",
    "    batch_size=6,\n",
    "    test_batch_size=6,\n",
    "    epochs=100,\n",
    "    cycle_consistency_loss=0.1,\n",
    "    feature_alignment_loss=0.1,\n",
    "    n_points=1024,\n",
    "    rot_factor=4,\n",
    "    exp_name=\"exp\",\n",
    "    n_subsampled_points=768,\n",
    "    model_path=\"\"\n",
    ")\n",
    "def _init_(args):\n",
    "    if not os.path.exists('checkpoints'):\n",
    "        os.makedirs('checkpoints')\n",
    "    if not os.path.exists('checkpoints/' + args.exp_name):\n",
    "        os.makedirs('checkpoints/' + args.exp_name)\n",
    "    if not os.path.exists('checkpoints/' + args.exp_name + '/' + 'models'):\n",
    "        os.makedirs('checkpoints/' + args.exp_name + '/' + 'models')\n",
    "    os.system('cp main.py checkpoints' + '/' + args.exp_name + '/' + 'main.py.backup')\n",
    "    os.system('cp model.py checkpoints' + '/' + args.exp_name + '/' + 'model.py.backup')\n",
    "    os.system('cp data.py checkpoints' + '/' + args.exp_name + '/' + 'data.py.backup')\n",
    "_init_(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PRNet(args).cuda()\n",
    "model_path = 'checkpoints' + '/' + args.exp_name + '/models/model.best.t7'\n",
    "state = torch.load(model_path)\n",
    "net.set_state(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ModelNet40(num_points=args.n_points,\n",
    "                          num_subsampled_points=args.n_subsampled_points,\n",
    "                          partition='test', gaussian_noise=False,\n",
    "                          unseen=True, rot_factor=args.rot_factor)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=args.test_batch_size, \n",
    "                         shuffle=False, \n",
    "                         drop_last=False)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        src, tgt, rotation_ab, translation_ab, rotation_ba, translation_ba, euler_ab, euler_ba = [d.cuda() for d in data]\n",
    "        rotation_ab_pred, translation_ab_pred = net.predict(src, tgt, n_iters=6)\n",
    "                \n",
    "        print(\"True transformation:\")\n",
    "        print(rotation_ab)\n",
    "        print(translation_ab)\n",
    "        print(\"------------------------\")\n",
    "        print(\"Predicted transformation:\")\n",
    "        print(rotation_ab_pred)\n",
    "        print(translation_ab_pred)\n",
    "        print(\"------------------------\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 768]) torch.Size([6, 3, 768])\n",
      "torch.Size([6, 3, 3]) torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "print(src.shape, tgt.shape)\n",
    "print(rotation_ab.shape, translation_ab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 40.4 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 17.9 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "nusc = NuScenes(version='v1.0-trainval', dataroot='/datasets_master/nuscenes', verbose=True)\n",
    "nuscenesHelper = NuScenesHelper(nusc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7783 samples loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_scenes = nuscenesHelper.get_small_validation_set()\n",
    "dataset = LidarDataset(nusc, test_scenes, skip=(1, 1), n_rounds=1, get_colors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1298/1298 [27:29<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    test_loader = DataLoader(dataset, \n",
    "                         batch_size=args.test_batch_size, \n",
    "                         shuffle=False, \n",
    "                         drop_last=False)\n",
    "    for data in tqdm(test_loader, position=0, leave=True):\n",
    "        src = data['source_points'].float().cuda() / 35.0\n",
    "        target = data['target_points'].cuda() / 35.0\n",
    "        \n",
    "        pred_r, pred_t = net.predict(src, target, n_iters=6)\n",
    "        pred_t *= 35.0\n",
    "        \n",
    "        pred_r = pred_r.cpu().detach().numpy()\n",
    "        pred_t = pred_t.cpu().detach().numpy()\n",
    "        true_r = data['rotation'].numpy()\n",
    "        true_t = data['translation'].numpy()\n",
    "        token1s = data['token1']\n",
    "        token2s = data['token2']\n",
    "        \n",
    "        for i in range(len(token1s)):\n",
    "            token1 = token1s[i]\n",
    "            token2 = token2s[i]\n",
    "            res = {\n",
    "                'true_rotation': true_r[i],\n",
    "                \"true_translation\": true_t[i],\n",
    "                \"predicted_rotation\": pred_r[i],\n",
    "                \"predicted_translation\": pred_t[i],\n",
    "                \"token1\": token1,\n",
    "                \"token2\": token2\n",
    "            }\n",
    "            filename = token1 + '_' + token2 + '.npy'\n",
    "            np.save(os.path.join('/root/no_backup/prnet_pretrained_modelnet40', filename), res) \n",
    "#             print(\"Saved: {}\".format(filename))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
